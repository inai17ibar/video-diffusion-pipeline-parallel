# パイプライン並列の性能分析

## 現状の実験結果の問題点

現在の実験では、**単一サンプルのレイテンシ**を測定しており、GPU数が増えるほど性能が悪化している。

### DummyUNet（シミュレータ）結果
| GPU数 | 合計時間 | 変化率 |
|-------|----------|--------|
| 1 | 3.93秒 | ベースライン |
| 2 | 4.55秒 | +16% 悪化 |
| 4 | 5.68秒 | +45% 悪化 |
| 7 | 7.23秒 | +84% 悪化 |

### SVD本番モード結果
| フレーム数 | GPU数 | 合計時間 |
|-----------|-------|----------|
| 2 | 1 | 4.8秒 |
| 14 | 7 | 8.4秒 |

---

## なぜ単一サンプルで性能が悪化するのか

### パイプライン並列の特性

パイプライン並列は**スループット最適化**の手法であり、**レイテンシ最適化**ではない。

```
単一サンプルの実行フロー:

時間 →
GPU0: [処理]→送信→待機.....................
GPU1:        受信→[処理]→送信→待機........
GPU2:               受信→[処理]→送信→待機..
...
GPU6:                              受信→[処理]→完了

合計時間 = Σ(各GPUの処理時間) + Σ(通信時間)
```

**単一サンプルでは:**
- 各GPUが順次処理するため、並列性がない
- GPU間通信のオーバーヘッドが加算される
- GPU数が増えるほど通信回数が増加し、悪化する

---

## パイプライン並列のメリットが出るケース

### 複数サンプル連続生成時

```
複数サンプルの実行フロー（定常状態）:

時間 →
GPU0: [S1][S2][S3][S4][S5][S6][S7]...
GPU1:    [S1][S2][S3][S4][S5][S6]...
GPU2:       [S1][S2][S3][S4][S5]...
GPU3:          [S1][S2][S3][S4]...
GPU4:             [S1][S2][S3]...
GPU5:                [S1][S2]...
GPU6:                   [S1]...

定常状態: 1GPUあたりのステップ時間ごとに1サンプルが完了
```

### 理論的な性能比較

| 構成 | 1サンプルのレイテンシ | 10サンプル合計 | スループット |
|------|----------------------|----------------|--------------|
| 1 GPU (28ステップ) | 28 × 150ms = 4.2秒 | 42秒 | 0.24 サンプル/秒 |
| 7 GPU (4ステップ/GPU) | ~8秒 | ~14秒 | **0.71 サンプル/秒** |

**7GPUでの10サンプル生成:**
- パイプライン充填: 7 × (4 × 150ms) ≈ 4.2秒
- 定常状態: 9サンプル × (4 × 150ms) ≈ 5.4秒
- 合計: 約10秒
- **スループット向上: 約3倍**

---

## パイプライン並列が有効なユースケース

### 適している
| ユースケース | 理由 |
|-------------|------|
| バッチ生成 | 複数サンプルで定常状態に到達 |
| リトライ生成 | 1枚の画像から複数候補を生成 |
| A/Bテスト | 異なるパラメータで複数生成 |
| メモリ制約 | 14フレームビデオは7GPU必須 |

### 適していない
| ユースケース | 理由 |
|-------------|------|
| 単発リクエスト | レイテンシが悪化 |
| リアルタイム応答 | 初回レイテンシが重要 |
| 少量生成 | パイプライン充填のオーバーヘッド |

---

## マルチサンプルスループット実測結果（2026-02-11）

前節の理論予測を `src.modes.benchmark` モジュール（DummyUNet + NCCL）で実測検証した。

### 実験環境

| 項目 | 値 |
|------|-----|
| GPU | NVIDIA RTX A5000 24GB x 7 |
| Backend | NCCL |
| Model | DummyUNet (Conv3d x 2 + LayerNorm) |
| Latent shape | `[1, 8, 16, 64, 64]` |
| Total steps | 28 |
| Warmup samples | 2 |
| Measured samples | 10 |
| Seed | 42 |

### 実行コマンド

```bash
# GPU数を変えてベンチマーク実行（1, 2, 4, 7 は 28 の約数）
torchrun --nproc_per_node=<NGPUS> -m src.modes.benchmark \
    --total-steps 28 --num-samples 10 --warmup-samples 2 \
    --hidden-channels <64|256> \
    --latent-channels 8 --latent-frames 16 --latent-height 64 --latent-width 64
```

### 実験1: hidden_channels=64（軽量モデル）

| GPU数 | Steps/GPU | Avg Sample Time (ms) | Throughput (samples/s) | Speedup | 効率 |
|-------|-----------|----------------------|------------------------|---------|------|
| 1     | 28        | 24.1                 | 41.57                  | 1.00x   | -    |
| 2     | 14        | 12.8                 | 78.28                  | 1.88x   | 94.1% |
| 4     | 7         | 6.9                  | 145.80                 | 3.51x   | 87.7% |
| 7     | 4         | 4.3                  | 231.93                 | 5.58x   | 79.7% |

初回サンプル時間（パイプライン充填コスト）:

| GPU数 | 初回サンプル (ms) | 2サンプル目以降 (ms) |
|-------|-------------------|----------------------|
| 1     | 179.3             | 24.1                 |
| 2     | 407.2             | 12.8                 |
| 4     | 1036.1            | 6.9                  |
| 7     | 1887.4            | 4.3                  |

### 実験2: hidden_channels=256（重いモデル）

| GPU数 | Steps/GPU | Avg Sample Time (ms) | Throughput (samples/s) | Speedup | 効率 |
|-------|-----------|----------------------|------------------------|---------|------|
| 1     | 28        | 96.0                 | 10.42                  | 1.00x   | -    |
| 2     | 14        | 49.0                 | 20.41                  | 1.96x   | 98.0% |
| 4     | 7         | 25.0                 | 39.97                  | 3.84x   | 95.9% |
| 7     | 4         | 15.1                 | 66.12                  | 6.35x   | 90.7% |

初回サンプル時間（パイプライン充填コスト）:

| GPU数 | 初回サンプル (ms) | 2サンプル目以降 (ms) |
|-------|-------------------|----------------------|
| 1     | 262.5             | 96.0                 |
| 2     | 444.1             | 49.0                 |
| 4     | 1021.5            | 25.0                 |
| 7     | 2020.1            | 15.1                 |

### Per-sample時間の詳細（hidden_channels=256, 7GPU）

```
Sample  1 (warmup): 2020.10 ms  ← パイプライン充填
Sample  2 (warmup):   15.28 ms
Sample  3:            15.16 ms
Sample  4:            15.12 ms
Sample  5:            15.11 ms
Sample  6:            15.11 ms
Sample  7:            15.11 ms
Sample  8:            15.11 ms
Sample  9:            15.12 ms
Sample 10:            15.12 ms
Sample 11:            15.12 ms
Sample 12:            15.16 ms
```

Warmup後のばらつきは **0.3% 未満**。定常状態では極めて安定したスループットを示す。

---

## 分析

### 1. 理論予測との比較

前節で「7GPUで約4.2xのスループット向上」と予測していたが、実測では:

- 軽量モデル: **5.58x**（予測を上回る）
- 重いモデル: **6.35x**（予測を大きく上回る）

DummyUNetはSVD UNetよりも軽量であるため、通信対計算比が異なり、予測値とは直接比較できない。
しかし、パイプライン並列の原理が正しく機能していることが実証された。

### 2. 計算負荷とスケーリング効率の関係

| hidden_channels | 1GPUでの1サンプル時間 | 7GPU効率 |
|-----------------|----------------------|----------|
| 64（軽量）      | 24.1 ms              | 79.7%    |
| 256（重い）     | 96.0 ms              | 90.7%    |

**計算負荷が大きいほどスケーリング効率が向上する。** これはGPU間通信（latentテンソルのsend/recv）のオーバーヘッドが固定的であるのに対し、各ステップの計算時間が増加するため、通信の相対コストが低下するためである。

SVD UNetは DummyUNet (hidden=256) よりもはるかに重い計算を行うため、本番環境ではさらに高い効率が期待できる。

### 3. パイプライン充填コスト

GPU数が増えるほど初回サンプルのレイテンシが増加する:

- 1GPU: 179 ms → 7GPU: 1887 ms（hidden=64）
- 1GPU: 263 ms → 7GPU: 2020 ms（hidden=256）

これはパイプラインの本質的な特性であり、全ステージが埋まるまでの遅延である。
ただし、2サンプル目以降は各ステージが同時に異なるサンプルを処理する定常状態に入り、スループットが急激に改善する。

### 4. スケーリング効率が線形に達しない要因

理論上限（7GPUで7.0x）に達しない主な要因:

1. **GPU間通信オーバーヘッド**: latentテンソル `[1, 8, 16, 64, 64]` (2MB fp32) のsend/recv
2. **パイプラインバブル**: 各GPUのステップ数の微小な不均衡
3. **CUDA同期コスト**: `torch.cuda.synchronize` による待機

---

## 結論

1. **パイプライン並列のスループット向上を実証** — 複数サンプル生成時に最大 6.35x の高速化（7GPU）
2. **単一サンプルではレイテンシ悪化** — 前回の実験結果と整合。パイプライン充填のコストが支配的
3. **計算負荷が大きいほど効率向上** — 重いモデルでは通信オーバーヘッドの相対コストが低下し、90%超の効率を達成
4. **定常状態のスループットは極めて安定** — warmup後のper-sample時間のばらつきは 0.3% 未満
5. **SVD本番環境ではさらに高い効率が期待** — DummyUNetよりも計算量が大きいため

---

## SVD実UNetによる実測検証（2026-02-11）

DummyUNetによる検証に加え、**実際のSVD UNet（5.7GB）**を使用した実測を行った。

### 実験環境

| 項目 | 値 |
|------|-----|
| GPU | NVIDIA RTX A5000 24GB x 7 |
| Backend | NCCL |
| Model | SVD UNet (UNetSpatioTemporalConditionModel, 5.7GB fp16) |
| Total steps | 7 |
| dtype | fp16 |
| Memory optimizations | Flash Attention + gradient checkpointing |

### 実験1: SVD UNet スループットスケーリング（ダミー条件付け）

`src.modes.production` でダミー条件付け（ランダムCLIP埋め込み + ランダム画像latent）を使用し、
純粋なUNetの推論性能を測定した。

#### メモリ制約の発見

| 構成 | Latent shape | 結果 |
|------|-------------|------|
| 1 GPU, 14フレーム | `[1, 4, 14, 32, 32]` | **OOM** |
| 1 GPU, 2フレーム | `[1, 4, 2, 32, 32]` | 成功（~130ms/step） |
| 7 GPU, 14フレーム | `[1, 4, 14, 32, 32]` | 成功（~155ms/step） |
| 7 GPU, 2フレーム | `[1, 4, 2, 32, 32]` | 成功（~134ms/step） |

**重要な発見: 1GPUでは14フレームのSVD推論が不可能。** SVD UNet（5.7GB）+ 14フレームの活性化メモリが24GB GPUの容量を超える。7GPU分散により、各GPUが1ステップのみ処理するため、活性化メモリのピークが抑制される。

#### 1 GPU ベースライン（2フレーム, 32x32 latent）

```
Sample 0: ~910ms (7 steps, 初回step 704ms + 残り6steps ~130ms/step)
Sample 1: ~910ms (7 steps, ~130ms/step)
Sample 2: ~910ms (7 steps, ~130ms/step)
```

Per-step定常状態: **~130ms/step**

#### 7 GPU スループット（2フレーム, 32x32 latent, 10サンプル）

```
Sample  0 (pipeline fill): ~6.5s (7 GPU × ~840ms初回step)
Sample  1:  136.10 ms
Sample  2:  132.24 ms
Sample  3:  286.09 ms (GC spike)
Sample  4:  132.52 ms
Sample  5:  133.21 ms
Sample  6:  134.19 ms
Sample  7:  134.58 ms
Sample  8:  133.79 ms
Sample  9:  134.26 ms
```

定常状態: **~134ms/sample** （パイプライン充填後）

#### スループット比較

| 構成 | Per-sample時間 | スループット | Speedup |
|------|---------------|-------------|---------|
| 1 GPU (7 steps) | 910 ms | 1.10 samples/s | 1.0x |
| 7 GPU (1 step/GPU) | 134 ms | 7.46 samples/s | **6.8x** |

**実際のSVD UNetで6.8xのスループット向上**を達成。DummyUNet (hidden=256) の6.35xを上回り、計算負荷が大きいほど効率が向上するという予測を裏付けた。

#### 7 GPU, 14フレーム（3サンプル）

14フレームは1GPUでは不可能だが、7GPUでは実行可能:

```
Sample 0 (pipeline fill): ~7.8s
Sample 1: 155.21 ms
Sample 2: 156.08 ms
```

14フレーム定常状態: **~155ms/sample**

### 実験2: SVD 実画像動画生成（フルパイプライン）

CLIP image encoder + VAE + SVD UNet の完全なパイプラインで、実際の画像から動画を生成した。

#### メモリ制約（フルパイプライン）

フルパイプラインでは各GPUに CLIP (~1.7GB) + VAE (~0.3GB) + UNet (~5.7GB) = **~7.7GB** のモデルが必要。

| 構成 | 解像度 | フレーム数 | 結果 |
|------|--------|-----------|------|
| 7 GPU | 576x1024 | 14 | **OOM** |
| 7 GPU | 320x576 | 14 | **OOM** |
| 7 GPU | 256x256 | 14 | **成功** |
| 7 GPU | 256x256 | 2 | 成功 |
| 1 GPU | 256x256 | 14 | **OOM** |
| 1 GPU | 256x256 | 2 | 成功 |

#### 成功した生成結果

| 構成 | 入力 | フレーム | 解像度 | モデルロード | Diffusion | Decode | 合計 |
|------|------|---------|--------|-------------|-----------|--------|------|
| 7 GPU | 実写真 | 14 | 256x256 | 10.57s | 4.88s | 0.34s | 15.78s |
| 7 GPU | テストパターン | 14 | 256x256 | 10.21s | 5.13s | 0.33s | 15.67s |
| 1 GPU | 実写真 | 2 | 256x256 | 7.62s | 1.04s | 0.11s | 8.77s |
| 1 GPU | テストパターン | 2 | 256x256 | 8.95s | 1.02s | 0.11s | 10.08s |

**7GPUのみが14フレーム動画生成に成功。1GPUでは14フレームはメモリ不足で生成不可能。**
入力画像の違い（実写真 vs テストパターン）による推論時間の差はほぼない。

#### 生成ファイル一覧

| ファイル | 説明 |
|---------|------|
| `outputs/experiment_svd/demo_input_photo_svd_7gpu_1770774419.mp4` | 7GPU, 14フレーム, 実写真入力 (メインデモ) |
| `outputs/experiment_svd/demo_input_photo_svd_7gpu_1770774419.gif` | 同上 GIF版 |
| `outputs/experiment_svd/demo_input_photo_svd_1gpu_1770774454.mp4` | 1GPU, 2フレーム, 実写真入力 (比較用) |
| `outputs/experiment_svd/demo_input_svd_7gpu_1770773348.mp4` | 7GPU, 14フレーム, テストパターン入力 |
| `outputs/experiment_svd/demo_input_svd_7gpu_1770773348.gif` | 同上 GIF版 |
| `outputs/experiment_svd/demo_input_svd_7gpu_1770773288.mp4` | 7GPU, 2フレーム, テストパターン入力 |
| `outputs/experiment_svd/demo_input_svd_1gpu_1770773629.mp4` | 1GPU, 2フレーム, テストパターン入力 |

### SVD実測からの結論

1. **メモリ制約の解消** — 1GPUでは14フレームSVD推論が不可能だが、7GPU分散により実行可能になる。これはスループット向上だけでなく、**そもそも実行不可能なワークロードを可能にする**という質的な利点
2. **スループット向上6.8x** — 実SVD UNetで理論限界（7.0x）に近い性能を達成。DummyUNet予測（6.35x）を上回る
3. **計算負荷とスケーリング効率の法則を確認** — DummyUNet (hidden=256, ~96ms/step) → SVD UNet (~130ms/step) と計算負荷が増すほど、通信オーバーヘッドの相対比が低下し、効率が向上
4. **フルパイプライン（CLIP+VAE+UNet）の制約** — 全モデルを各GPUにロードするため、高解像度でのメモリ制約が厳しい。改善策として、CLIP/VAEをrank 0のみにロードする最適化が考えられる

---

## SVD動画品質改善 + ネイティブ解像度達成（2026-02-11）

前回実験では動画が真っ白/破綻していた問題を修正し、SVDネイティブ解像度（1024x576）での動画生成に成功した。

### 修正内容

| 修正 | 問題 | 効果 |
|------|------|------|
| `@torch.inference_mode()` 追加 | forward()で勾配追跡が有効、メモリ2倍消費 | メモリ使用量を約半減、ネイティブ解像度が動作可能に |
| `init_noise_sigma` スケーリング | 初期ノイズが未スケーリング（std=1.0）、正しくはstd≈10.5 | **白い出力の根本原因を解決** |
| Attention slicing + memory optimizations | UNetのattentionメモリが大きい | ピークメモリを追加で削減 |
| `torch.cuda.empty_cache()` 各ステップ後 | 複数ステップ/GPUでメモリ蓄積 | multi-step/GPUの安定性向上 |
| `EulerDiscreteScheduler` 直接使用 | 手動実装のスケジューラが不正確 | v_prediction + Karras sigmasの正確な実装 |
| Classifier-Free Guidance (CFG) 実装 | CFG未実装で品質低下 | per-frame guidance scaleによる品質向上 |
| 画像前処理をセンタークロップに変更 | リサイズによるアスペクト比歪み | 入力画像の品質維持 |

### 白い出力の原因分析

**根本原因: `init_noise_sigma` による初期ノイズのスケーリング欠落**

diffusersの公式SVDパイプラインでは、初期ノイズを `scheduler.init_noise_sigma` でスケーリングする:
```python
# 公式 diffusers
latents = randn_tensor(shape, ...) * scheduler.init_noise_sigma  # ≈ 10.47
```

我々のコードでは `randn()` そのまま（std=1.0）を使用していたため:
- UNetが約10倍小さいノイズを入力として受け取る
- v-predictionが近似的にゼロを予測
- デノイジングが定数（VAEデコード後に白）に収束
- ステップ数が多いほど収束が進み、より白くなる

| ステップ数 | 修正前 mean | 修正後 mean | 修正前 std | 修正後 std |
|-----------|------------|------------|-----------|-----------|
| 7 (CFG=3.0) | 69.6 | 155.4 | 45.9 | 61.9 |
| 21 (CFG=1.0) | 245.4 | 165.8 | 8.6 | 88.1 |
| 21 (CFG=3.0) | 241.8 | 170.0 | 10.5 | 83.8 |
| 25f, 21 (CFG=3.0) | 230.7 | 90.1 | 18.7 | 94.1 |

### 実験結果（修正後）

#### 実験環境

| 項目 | 値 |
|------|-----|
| GPU | NVIDIA RTX A5000 24GB x 7 |
| Backend | NCCL |
| Model | SVD UNet (fp16) + CLIP + VAE |
| Memory optimizations | inference_mode + attention slicing + xformers + cache clearing |
| 入力画像 | 実写真 (demo_input_photo.jpg)、センタークロップ |

#### 全実験結果

| Resolution | Frames | Steps | CFG | Diffusion | Decode | Total | Output Dir |
|-----------|--------|-------|-----|-----------|--------|-------|------------|
| 1024x576 | 14 | 21 | 1.0 | 24.2s | 2.6s | 37.0s | `exp_v2_14f_576x1024_21steps_noCFG/` |
| 1024x576 | 14 | 7 | 3.0 | 16.6s | 2.6s | 29.7s | `exp_v2_14f_576x1024_7steps_cfg3/` |
| 1024x576 | 14 | 21 | 3.0 | 43.5s | 2.6s | 56.7s | `exp_v2_14f_576x1024_21steps_cfg3/` |
| 1024x576 | 25 | 21 | 3.0 | 72.5s | 4.9s | 88.1s | `exp_v2_25f_576x1024_21steps_cfg3_5sec/` |

#### ピクセル統計（修正後の正常な出力）

| 設定 | min | max | mean | std |
|------|-----|-----|------|-----|
| 14f, 21steps, CFG=3.0 | 0 | 255 | 170.0 | 83.8 |
| 14f, 21steps, no CFG | 0 | 255 | 165.8 | 88.1 |
| 14f, 7steps, CFG=3.0 | 0 | 255 | 155.4 | 61.9 |
| 25f, 21steps, CFG=3.0 | 0 | 255 | 90.1 | 94.1 |

全設定でmin=0, max=255の正常な範囲。

### 5秒動画生成

SVD-XTモデルは最大25フレームをサポート。fps=5に設定することで25フレーム ÷ 5fps = **5秒の動画**を生成可能。

```bash
torchrun --nproc_per_node=7 scripts/generate_video_demo.py \
    --input-image demo_input_photo.jpg \
    --total-steps 21 --num-frames 25 --height 576 --width 1024 \
    --guidance-scale 3.0 --fps 5 --seed 42
```

結果: 1024x576, 25フレーム, 5fps = 5秒動画、生成時間88.1秒

### メモリ最適化の効果

| 最適化 | 効果 |
|--------|------|
| `@torch.inference_mode()` | **最大のブレークスルー** — 勾配追跡無効化でメモリ約半減 |
| CLIP/VAE早期解放 | 非最終rankでVAEを削除、全rankでCLIPを削除 |
| Attention slicing | UNet内のattentionをスライス実行 |
| `torch.cuda.empty_cache()` | ステップ間のメモリフラグメンテーション解消 |

これらの最適化により、以前OOMだった全設定（576x1024 + 14/25フレーム + CFG）が動作可能になった。

### 結論

1. **白い出力の問題を解決** — `init_noise_sigma` スケーリングの欠落が根本原因
2. **ネイティブ解像度達成** — SVDの設計解像度1024x576で14/25フレーム動画を生成
3. **CFGによる品質向上** — guidance_scale=3.0でper-frame guidanceを適用
4. **5秒動画生成が可能** — 25フレーム + fps=5で5秒の動画を生成（88秒で完了）
5. **`@torch.inference_mode()`が最重要** — 推論時には必須。これ無しではネイティブ解像度は不可能

---

## GPUスケーリング実験: 何台まで意味があるか（2026-02-11）

GPU台数を1, 3, 5, 7と変化させ、同一条件でパイプライン並列のスケーリング特性を計測した。

### 実験環境

| 項目 | 値 |
|------|-----|
| GPU | NVIDIA RTX A5000 24GB × 7 |
| Backend | NCCL |
| Model | SVD UNet (fp16, no CFG) |
| Latent shape | `[1, 4, 14, 40, 72]` (320×576, 14 frames) |
| Total steps | 105 (= LCM(1,3,5,7), 全GPU数で均等分割可能) |
| Warmup samples | 2 |
| Measured samples | 5 |
| Memory optimizations | xformers + attention slicing + inference_mode |

### 実行コマンド

```bash
torchrun --nproc_per_node=<N> -m src.modes.benchmark \
    --model svd --total-steps 105 \
    --latent-channels 4 --latent-frames 14 \
    --latent-height 40 --latent-width 72 \
    --warmup-samples 2 --num-samples 5
```

### スループット・レイテンシ結果

| GPU数 | Steps/GPU | 1本目レイテンシ | 定常per-sample | スループット | Speedup | 効率 |
|-------|-----------|----------------|---------------|-------------|---------|------|
| 1     | 105       | 31.51s         | 31.96s        | 0.031 samples/s | 1.00x | -    |
| 3     | 35        | 32.77s         | 10.54s        | 0.095 samples/s | 3.03x | 101% |
| 5     | 21        | 34.11s         | 6.13s         | 0.163 samples/s | 5.21x | 104% |
| 7     | 15        | 35.30s         | 4.39s         | 0.228 samples/s | 7.28x | 104% |

効率が100%を超えるのは、GPU数が増えるとステップ/GPUが減りキャッシュ効率が向上するためと考えられる。

### Per-sample時間の詳細

#### 1 GPU (105 steps/GPU)

```
Sample 1: 31511.6 ms (= 全105 steps直列)
Sample 2: 31319.4 ms
Sample 3: 31699.9 ms  ← measured
Sample 4: 31903.4 ms
Sample 5: 32033.0 ms
Sample 6: 32081.4 ms
Sample 7: 32100.3 ms
```

1GPUでは全サンプルが同じ時間（~32s）。パイプライン重ね合わせなし。

#### 7 GPU (15 steps/GPU)

```
Sample 1: 35303.3 ms (パイプライン充填: 7 stages × 15 steps × ~290ms)
Sample 2:  4375.8 ms (定常状態突入)
Sample 3:  4399.0 ms  ← measured
Sample 4:  4394.9 ms
Sample 5:  4370.5 ms
Sample 6:  4384.4 ms
Sample 7:  4394.2 ms
```

1本目の35.3sに対し、2本目以降は4.4sで安定。**パイプライン充填後のスループットは8.0倍**。

### GPU利用率

パイプライン並列では各ステージが順番に処理するため、GPU利用率に特徴的なパターンが現れる。

#### 1 GPU

```
gpu0: 100% (常時)
```

1台のGPUが全105 stepsを連続処理。利用率は100%で最も効率的。

#### 7 GPU（パイプライン充填フェーズ、3秒間隔でサンプリング）

```
時刻  gpu0  gpu1  gpu2  gpu3  gpu4  gpu5  gpu6
 0s:  100%    0%    0%    0%    0%    0%    0%   ← rank0が15 steps処理中
 3s:  100%  100%    0%    0%    0%    0%    0%   ← rank0→1に渡し中
 6s:   99%   99%    0%    0%    0%    0%    0%
 9s:  100%  100%  100%    0%    0%    0%    0%   ← rank2が処理開始
12s:    0%  100%  100%   97%    0%    0%    0%   ← rank0は処理完了、idle
15s:    0%   31%  100%  100%    0%    0%    1%
18s:    0%    0%   99%  100%   99%    0%    0%
21s:    7%    0%    8%   99%   54%    0%    0%
24s:    0%    0%    0%  100%  100%   99%    0%
27s:    0%    0%    0%    0%  100%   99%   72%  ← rank6が処理開始
```

**観察:**
- 各GPUは自分のステージを処理中のみ100%、それ以外は0%
- **同時にアクティブなGPUは最大2-3台**（send/recv遷移のオーバーラップ）
- 処理済みGPUのメモリは解放されないが、計算はidle

#### 定常状態のGPU利用率

定常状態（2本目以降）でも本質は同じ。各GPUが順番に担当stepを処理し、終わったら次のサンプルを待つ。
**7台中常に1-2台のみがアクティブで、残り5-6台はidle。**

これはパイプライン並列の本質的な制約であり、GPU台数を増やしても解消しない。

### 損益分岐点分析

7 GPU構成のパイプライン充填コスト（~35s）を償却するために必要なサンプル数:

```
1 GPU: N × 31.96s
7 GPU: 35.30s + (N-1) × 4.39s

等式: 31.96N = 35.30 + 4.39(N-1)
      31.96N = 30.91 + 4.39N
      27.57N = 30.91
      N ≈ 1.12
```

**2本目以降は常に7 GPUが速い。** これはper-step時間（~290ms）に対して通信オーバーヘッドが極めて小さいため。

N本の合計生成時間:

| N (本数) | 1 GPU | 7 GPU | 7 GPUの優位性 |
|----------|-------|-------|--------------|
| 1        | 32.0s | 35.3s | -10% (遅い) |
| 2        | 63.9s | 39.7s | **+38%** |
| 5        | 159.8s | 52.9s | **+67%** |
| 10       | 319.6s | 74.8s | **+77%** |
| 50       | 1598s  | 250s  | **+84%** |

### 結論

1. **スループットはほぼ理想的な線形スケーリング** — 7 GPUで7.28x（理論上限7.0xを超過）。SVD UNetの計算量が大きいため、通信オーバーヘッドが相対的に無視できるレベル

2. **1本目レイテンシはGPU数によらず一定（~32-35s）** — パイプライン充填で全stepsを直列通過するため改善不可能。レイテンシ重視の単発リクエストにはGPU追加のメリットなし

3. **GPU利用率は常に低い（7台中1-2台のみアクティブ）** — パイプライン並列の本質的制約。リソース効率の観点からは、アイドルGPUを他ワークロードと共有（NVIDIA MPS等）する設計が望ましい

4. **2本以上の連続生成なら7 GPUが常に有利** — 損益分岐点はわずか1.12本。バッチ生成ユースケースではGPU台数を最大化すべき

5. **GPU台数の上限** — 今回の105 stepsでは7 GPUまで効率低下なし。steps/GPU=15でもオーバーヘッドが顕在化していないため、さらに多くのGPU（例: 15台, 7 steps/GPU）でも効率を維持できる可能性がある。ボトルネックはsteps/GPUあたりの計算時間がsend/recvの通信時間を下回る点（推定: steps/GPU < 3-5程度で効率低下開始）

---

## 最適ステージ数の理論分析

### 何が最適ステージ数を決めるか

パイプライン並列の定常スループットは、**ステージ処理時間 vs GPU間通信時間**の大きい方で律速される:

```
T_stage = (S / K) × T_step    ... K台のGPUがそれぞれ S/K ステップを処理
T_comm  = latent_size / bandwidth  ... GPU間のlatent転送時間

定常per-sample = max(T_stage, T_comm)
```

スケーリングが破綻する（GPU追加が無意味になる）条件:

```
T_stage ≈ T_comm
→ (S / K) × T_step ≈ T_comm
→ K_max ≈ S × T_step / T_comm
```

### SVD UNetの計算・通信比

| 項目 | 値 |
|------|-----|
| T_step (1 UNet forward, fp16, 320×576, 14 frames) | ~290 ms |
| Latent tensor size ([1,4,14,40,72] fp16) | 0.6 MB |
| PCIe Gen4 x16 bandwidth | ~32 GB/s |
| T_comm (推定) | ~0.02 ms |
| **計算/通信比** | **~14,500 : 1** |

通信が律速になるK:

```
K_max ≈ S × T_step / T_comm = 105 × 290ms / 0.02ms ≈ 1,500,000
```

つまり**通信は事実上ゼロコスト**であり、ステージ数の理論上限は通信では決まらない。
今回の7 GPUで効率104%が出た（通信ペナルティなし）のはこの圧倒的な比率のため。

### 実質的な制約（通信以外）

通信が効かないため、最適ステージ数は以下の実用的制約で決まる:

| 制約 | 内容 | 影響 |
|------|------|------|
| **ステップ分割可能性** | `total_steps % K == 0` が必須 | 25 steps → K ∈ {1, 5, 25} のみ |
| **ステップ数上限** | K ≤ total_steps（1GPU最低1step） | 25 steps → 最大25 GPU |
| **メモリ** | 全GPUにUNet全体をロード（~5.7GB fp16） | 活性化メモリを含め24GB GPU 1台で320×576が限界 |
| **パイプライン充填** | 1本目レイテンシ = S × T_step（K無関係） | 少数サンプル生成ではK増加の恩恵薄い |
| **コスト** | K台分のGPUメモリ・電力コスト | GPU利用率は1/K程度で非効率 |

### 通信が律速になるケース

SVD UNetでは通信が無視できるが、以下の条件では顕在化する:

1. **軽量モデル**: T_step が小さいと比率が縮小（例: DummyUNet hidden=64 では T_step ≈ 0.8ms、比率は~40:1）
2. **高解像度 latent**: 1024×576 → latent [1,4,14,72,128] = 4.1MB、T_comm ≈ 0.13ms（それでも比率は~2,200:1）
3. **低帯域接続**: マルチノード（InfiniBand 200Gbps ≈ 25GB/s）やEthernet（10Gbps ≈ 1.25GB/s）では T_comm が10-100倍に増加
4. **steps/GPU が極端に少ない**: 1 step/GPU で T_stage = 290ms、まだ余裕だが、モデルの初期化・同期等の固定オーバーヘッドが顕在化する可能性
